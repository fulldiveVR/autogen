{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory \n",
    "\n",
    "There are several use cases where it is valuable to maintain a _store_ of useful facts that can be intelligently added to the context of the agent just before a specific step. The typically use case here is a RAG pattern where a query is used to retrieve relevant information from a database that is then added to the agent's context.\n",
    "\n",
    "\n",
    "AgentChat provides a {py:class}`~autogen_agentchat.memory.Memory` protocol that can be extended to provide this functionality.  The key methods are `query`, `transform`,  `add`, `clear`, and `cleanup`. \n",
    "\n",
    "- `query`: retrieve relevant information from the memory store \n",
    "- `transform`: mutate an agent's internal `model_context` by adding the retrieved information (used in the {py:class}`~autogen_agentchat.agents.AssistantAgent` class) \n",
    "- `add`: add new entries to the memory store\n",
    "- `clear`: clear all entries from the memory store\n",
    "- `cleanup`: clean up any resources used by the memory store  \n",
    "\n",
    "\n",
    "## ListMemory Example\n",
    "\n",
    "{py:class}`~autogen_agentchat.memory.ListMemory` is provided as an example implementation of the {py:class}`~autogen_agentchat.memory.Memory` protocol. It is a simple list-based memory implementation that uses text similarity matching to retrieve relevant information from the memory store. The similarity score is calculated using the `SequenceMatcher` class from the `difflib` module. The similarity score is calculated between the query text and the content text of each memory entry.   \n",
    "\n",
    "In the following example, we will use ListMemory to similate a memory bank of user preferences and explore how it might be used in personalizing the agent's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.memory import ListMemory, MemoryContent, MemoryMimeType\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize user memory\n",
    "user_memory = ListMemory()\n",
    "\n",
    "# Add user preferences to memory\n",
    "await user_memory.add(MemoryContent(content=\"The weather should be in metric units\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "await user_memory.add(MemoryContent(content=\"Meal recipe must be vegan\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "\n",
    "async def get_weather(city: str, units: str = \"imperial\") -> str:\n",
    "    if units == \"imperial\":\n",
    "        return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "    elif units == \"metric\":\n",
    "        return f\"The weather in {city} is 23 degrees and Sunny.\"\n",
    "\n",
    "\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"assistant_agent\",\n",
    "    model_client=OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "    ),\n",
    "    tools=[get_weather],\n",
    "    memory=[user_memory],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n",
      "---------- assistant_agent ----------\n",
      "[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=0.463768115942029)]\n",
      "---------- assistant_agent ----------\n",
      "[FunctionCall(id='call_OkQ4Z7u2RZLU6dA7GTAQiG9j', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n",
      "[Prompt tokens: 128, Completion tokens: 20]\n",
      "---------- assistant_agent ----------\n",
      "[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_OkQ4Z7u2RZLU6dA7GTAQiG9j')]\n",
      "---------- assistant_agent ----------\n",
      "The weather in New York is 23 degrees and Sunny.\n",
      "---------- Summary ----------\n",
      "Number of messages: 5\n",
      "Finish reason: None\n",
      "Total prompt tokens: 128\n",
      "Total completion tokens: 20\n",
      "Duration: 0.80 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=0.463768115942029)], type='MemoryQueryEvent'), ToolCallRequestEvent(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=128, completion_tokens=20), content=[FunctionCall(id='call_OkQ4Z7u2RZLU6dA7GTAQiG9j', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant_agent', models_usage=None, content=[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_OkQ4Z7u2RZLU6dA7GTAQiG9j')], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant_agent', models_usage=None, content='The weather in New York is 23 degrees and Sunny.', type='ToolCallSummaryMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the agent with a task.\n",
    "stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect that the `assistant_agent` model_context is actually updated with the retrieved memory entries.  The `transform` method is used to format the retrieved memory entries into a string that can be used by the agent.  In this case, we simply concatenate the content of each memory entry into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserMessage(content='What is the weather in New York?', source='user', type='UserMessage'),\n",
       " SystemMessage(content='\\n The following results were retrieved from memory for this task. You may choose to use them or not. :\\n1. The weather should be in metric units\\n', type='SystemMessage'),\n",
       " AssistantMessage(content=[FunctionCall(id='call_OkQ4Z7u2RZLU6dA7GTAQiG9j', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], source='assistant_agent', type='AssistantMessage'),\n",
       " FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_OkQ4Z7u2RZLU6dA7GTAQiG9j')], type='FunctionExecutionResultMessage')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant_agent._model_context.get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'),\n",
       " MemoryQueryEvent(source='assistant_agent', models_usage=None, content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=0.463768115942029)], type='MemoryQueryEvent'),\n",
       " ToolCallRequestEvent(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=219, completion_tokens=20), content=[FunctionCall(id='call_YPwxZOz0bTEW15beow3zXsaI', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], type='ToolCallRequestEvent'),\n",
       " ToolCallExecutionEvent(source='assistant_agent', models_usage=None, content=[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_YPwxZOz0bTEW15beow3zXsaI')], type='ToolCallExecutionEvent'),\n",
       " ToolCallSummaryMessage(source='assistant_agent', models_usage=None, content='The weather in New York is 23 degrees and Sunny.', type='ToolCallSummaryMessage')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await assistant_agent.run(task=\"What is the weather in New York?\")\n",
    "result.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that the weather is returned in Centigrade as stated in the user preferences. \n",
    "\n",
    "Similarly, assuming we ask a separate question about generating a meal plan, the agent is able to retrieve relevant information from the memory store and provide a personalized response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write brief meal recipe with broth\n",
      "---------- assistant_agent ----------\n",
      "[MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=0.5084745762711864)]\n",
      "---------- assistant_agent ----------\n",
      "Here's a brief vegan recipe using broth:\n",
      "\n",
      "**Vegan Vegetable Noodle Soup**\n",
      "\n",
      "**Ingredients:**\n",
      "- 4 cups vegetable broth\n",
      "- 1 cup water\n",
      "- 1 cup carrots, sliced\n",
      "- 1 cup celery, chopped\n",
      "- 1 cup noodles (such as rice noodles or spaghetti broken into smaller pieces)\n",
      "- 2 cups kale or spinach, chopped\n",
      "- 2 cloves garlic, minced\n",
      "- 1 tablespoon olive oil\n",
      "- Salt and pepper to taste\n",
      "- Lemon juice (optional)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Sauté Vegetables:** In a large pot, heat olive oil over medium heat. Add minced garlic and sauté until fragrant. Add carrots and celery, and sauté for about 5 minutes, until they start to soften.\n",
      "\n",
      "2. **Add Broth and Noodles:** Pour in the vegetable broth and water, bringing it to a boil. Add the noodles and cook according to package instructions until they are al dente.\n",
      "\n",
      "3. **Cook Greens:** Stir in the kale or spinach and allow it to simmer for a couple of minutes until wilted.\n",
      "\n",
      "4. **Season and Serve:** Season with salt and pepper to taste. If desired, add a squeeze of lemon juice for extra flavor. \n",
      "\n",
      "5. **Enjoy:** Serve hot and enjoy your nutritious, comforting soup!\n",
      "\n",
      "This simple, flavorful soup is not only vegan but also packed with nutrients, making it a perfect meal any day. \n",
      "\n",
      "TERMINATE\n",
      "[Prompt tokens: 306, Completion tokens: 294]\n",
      "---------- Summary ----------\n",
      "Number of messages: 3\n",
      "Finish reason: None\n",
      "Total prompt tokens: 306\n",
      "Total completion tokens: 294\n",
      "Duration: 4.39 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write brief meal recipe with broth', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, content=[MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=0.5084745762711864)], type='MemoryQueryEvent'), TextMessage(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=306, completion_tokens=294), content=\"Here's a brief vegan recipe using broth:\\n\\n**Vegan Vegetable Noodle Soup**\\n\\n**Ingredients:**\\n- 4 cups vegetable broth\\n- 1 cup water\\n- 1 cup carrots, sliced\\n- 1 cup celery, chopped\\n- 1 cup noodles (such as rice noodles or spaghetti broken into smaller pieces)\\n- 2 cups kale or spinach, chopped\\n- 2 cloves garlic, minced\\n- 1 tablespoon olive oil\\n- Salt and pepper to taste\\n- Lemon juice (optional)\\n\\n**Instructions:**\\n\\n1. **Sauté Vegetables:** In a large pot, heat olive oil over medium heat. Add minced garlic and sauté until fragrant. Add carrots and celery, and sauté for about 5 minutes, until they start to soften.\\n\\n2. **Add Broth and Noodles:** Pour in the vegetable broth and water, bringing it to a boil. Add the noodles and cook according to package instructions until they are al dente.\\n\\n3. **Cook Greens:** Stir in the kale or spinach and allow it to simmer for a couple of minutes until wilted.\\n\\n4. **Season and Serve:** Season with salt and pepper to taste. If desired, add a squeeze of lemon juice for extra flavor. \\n\\n5. **Enjoy:** Serve hot and enjoy your nutritious, comforting soup!\\n\\nThis simple, flavorful soup is not only vegan but also packed with nutrients, making it a perfect meal any day. \\n\\nTERMINATE\", type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = assistant_agent.run_stream(task=\"Write brief meal recipe with broth\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Memory Stores (Vector DBs, etc.)\n",
    "\n",
    "You can build on the `Memory` protocol to implement more complex memory stores. For example, you could implement a custom memory store that uses a vector database to store and retrieve information, or a memory store that uses a machine learning model to generate personalized responses based on the user's preferences etc.\n",
    "\n",
    "Specifically, you will need to overload the  `query`, `transform`, and `add` methods to implement the desired functionality and pass the memory store to your agent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
